{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fambargh/SAMPLE/blob/master/Untitled19-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwLPDqfN-knk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "59994f0b-95e5-4d2c-9f0a-1fe845c3d846"
      },
      "source": [
        "pip install Pillow\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BunJzspDblf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "9d4bf4f8-cef2-4310-e40a-96b84bb3aee9"
      },
      "source": [
        "pip install scipy==1.1.0\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.16.4)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.3.0\n",
            "    Uninstalling scipy-1.3.0:\n",
            "      Successfully uninstalled scipy-1.3.0\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXkKAyskDbtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.misc import imread, imresize, imsave\n",
        "import time\n",
        "from scipy import misc, ndimage\n",
        "import random\n",
        "from scipy.signal import medfilt\n",
        "import cv2\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm0L6pNLDbwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convlayers():\n",
        "\n",
        "\t\tprint('\\nconvlayers(): Initializing layers')\n",
        "\n",
        "\t\tparameters = []\n",
        "\n",
        "\t\t# normalize input\n",
        "\t\twith tf.name_scope('preprocess') as scope:\n",
        "\t\t\timages = imgs/255 - 0.5\n",
        "\n",
        "\t\twith tf.name_scope('conv1_1') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 1, 8], name='weights1_1')\n",
        "\t\t\tconv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[8], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tconv1_1 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\tpool1 = tf.nn.max_pool(conv1_1,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool1')\n",
        "\n",
        "\t\tdropout1 = tf.layers.dropout(pool1,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=train_mode,\n",
        "\t\t\t                         name='dropout1')\n",
        "\n",
        "\t\twith tf.name_scope('conv1_2') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 8, 16], name='weights1_2')\n",
        "\t\t\tconv = tf.nn.conv2d(dropout1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[16], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tconv1_2 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\tpool2 = tf.nn.max_pool(conv1_2,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool2')\n",
        "\n",
        "\t\tsdropout2 = tf.layers.dropout(pool2,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=train_mode,\n",
        "\t\t\t                         name='dropout2')\n",
        "\n",
        "\t\twith tf.name_scope('conv1_3') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 16, 32], name='weights1_3')\n",
        "\t\t\tconv = tf.nn.conv2d(dropout2, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[32], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tconv1_3 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\tpool3 = tf.nn.max_pool(conv1_3,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool3')\n",
        "\n",
        "\t\tdropout3 = tf.layers.dropout(pool3,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=train_mode,\n",
        "\t\t\t                         name='dropout3')\n",
        "\n",
        "\t\twith tf.name_scope('conv1_4') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[1, 1, 32, 16], name='weights1_4')\n",
        "\t\t\tconv = tf.nn.conv2d(dropout3, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[16], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tconv1_4 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\twith tf.name_scope('conv1_6') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[1, 1, 16, 2], name='weights1_6')\n",
        "\t\t\tconv = tf.nn.conv2d(conv1_4, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[2], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tconv1_6 = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\tsaver = tf.train.Saver({'W1': parameters[0], 'b1': parameters[1], \n",
        "\t\t\t                         'W2': parameters[2], 'b2': parameters[3], \n",
        "\t\t\t                         'W3': parameters[4], 'b3': parameters[5], \n",
        "\t\t\t                         'W4': parameters[6], 'b4': parameters[7], \n",
        "\t\t\t                         'W5': parameters[8], 'b5': parameters[9]})\n",
        "\n",
        "\t\tprint('output: ', np.shape(conv1_6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCbAenZPDt_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_patches( image, patchshape, overlap_allowed=0.5, cropvalue=None,\n",
        "\t\t\t\t\tcrop_fraction_allowed=0.1):\n",
        "\t\t\"\"\"\n",
        "\t\tGiven an image, extract patches of a given shape with a certain\n",
        "\t\tamount of allowed overlap between patches, using a heuristic to\n",
        "\t\tensure maximum coverage.\n",
        "\t\tIf cropvalue is specified, it is treated as a flag denoting a pixel\n",
        "\t\tthat has been cropped. Patch will be rejected if it has more than\n",
        "\t\tcrop_fraction_allowed * prod(patchshape) pixels equal to cropvalue.\n",
        "\t\tLikewise, patches will be rejected for having more overlap_allowed\n",
        "\t\tfraction of their pixels contained in a patch already selected.\n",
        "\t\t\"\"\"\n",
        "\t\tjump_cols = int(patchshape[1] * overlap_allowed)\n",
        "\t\tjump_rows = int(patchshape[0] * overlap_allowed)\n",
        "\t\t\n",
        "\t\t# Restrict ourselves to the rectangle containing non-cropped pixels\n",
        "\t\tif cropvalue is not None:\n",
        "\t\t\trows, cols = np.where(image != cropvalue)\n",
        "\t\t\trows.sort(); cols.sort()\n",
        "\t\t\tactive =  image[rows[0]:rows[-1], cols[0]:cols[-1]]\n",
        "\t\telse:\n",
        "\t\t\tactive = image\n",
        "\n",
        "\t\trowstart = 0; colstart = 0\n",
        "\n",
        "\t\t# Array tracking where we've already taken patches.\n",
        "\t\tcovered = np.zeros(active.shape, dtype=bool)\n",
        "\t\tpatches = []\n",
        "\n",
        "\t\twhile rowstart < active.shape[0] - patchshape[0]:\n",
        "\t\t\t# Record whether or not e've found a patch in this row, \n",
        "\t\t\t# so we know whether to skip ahead.\n",
        "\t\t\tgot_a_patch_this_row = False\n",
        "\t\t\tcolstart = 0\n",
        "\t\t\twhile colstart < active.shape[1] - patchshape[1]:\n",
        "\t\t\t\t# Slice tuple indexing the region of our proposed patch\n",
        "\t\t\t\tregion = (slice(rowstart, rowstart + patchshape[0]),\n",
        "\t\t\t\t\t\t  slice(colstart, colstart + patchshape[1]))\n",
        "\t\t\t\t\n",
        "\t\t\t\t# The actual pixels in that region.\n",
        "\t\t\t\tpatch = active[region]\n",
        "\n",
        "\t\t\t\t# The current mask value for that region.\n",
        "\t\t\t\tcover_p = covered[region]\n",
        "\t\t\t\tif cropvalue is None or \\\n",
        "\t\t\t\t   frac_eq_to(patch, cropvalue) <= crop_fraction_allowed and \\\n",
        "\t\t\t\t   frac_eq_to(cover_p, True) <= overlap_allowed:\n",
        "\t\t\t\t\t# Accept the patch.\n",
        "\t\t\t\t\tpatches.append(patch)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Mask the area.\n",
        "\t\t\t\t\tcovered[region] = True\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Jump ahead in the x direction.\n",
        "\t\t\t\t\tcolstart += jump_cols\n",
        "\t\t\t\t\tgot_a_patch_this_row = True\n",
        "\t\t\t\t\t#print \"Got a patch at %d, %d\" % (rowstart, colstart)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# Otherwise, shift window across by one pixel.\n",
        "\t\t\t\t\tcolstart += 1\n",
        "\n",
        "\t\t\tif got_a_patch_this_row:\n",
        "\t\t\t\t# Jump ahead in the y direction.\n",
        "\t\t\t\trowstart += jump_rows\n",
        "\t\t\telse:\n",
        "\t\t\t\t# Otherwise, shift the window down by one pixel.\n",
        "\t\t\t\trowstart += 1\n",
        "\n",
        "\t\t\t# Return a 3D array of the patches with the patch index as the first\n",
        "\t\t\t# dimension (so that patch pixels stay contiguous in memory, in a \n",
        "\t\t\t# C-ordered array).\n",
        "\n",
        "\t\treturn np.concatenate([pat[np.newaxis, ...] for pat in patches], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpSqmvgwDujx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess( images):\n",
        "\n",
        "\t\tnew_images = []\n",
        "\n",
        "\t\tfor i, img in enumerate(images):\n",
        "\n",
        "\t\t\tclahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(40,40))\n",
        "\t\t\timg = clahe.apply(img)\n",
        "\n",
        "\t\t\timg = cv2.bilateralFilter(img, -1, 20, 20)\n",
        "\n",
        "\t\t\tintensities = medfilt(img, (21, 21))\n",
        "\t\t\tintensities = intensities.astype(np.float32)\n",
        "\t\t\tintensities_smoothed = cv2.bilateralFilter(intensities, -1, 70, 13)\n",
        "\t\t\twidth, height = img.shape\n",
        "\t\t\timg[0:width, 0:height] = img[0:width, 0:height] + (90) - intensities_smoothed[0:width, 0:height]\n",
        "\t\t\tidx = img[:] > 210\n",
        "\t\t\timg[idx] = 18\n",
        "\n",
        "\t\t\tclahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(1,1))\n",
        "\t\t\timg = clahe.apply(img)\n",
        "\n",
        "\t\t\tnew_images.append(img)\n",
        "\n",
        "\t\treturn np.array(new_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAbP8bUUDuxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_test_img( sess, idx, img, gt):\n",
        "\n",
        "\t\ttest = img[np.newaxis, ... ]\n",
        "\n",
        "\t\tpred = sess.run(output, feed_dict={\n",
        "\t\t\t\t\t\t\timgs: test, \n",
        "\t\t\t\t\t\t\ttrain_mode: False\n",
        "\t\t\t\t\t\t})\n",
        "\t\tprint('TEST_set[', idx, ']:')\n",
        "\t\tprint('gr_t: ', gt)\n",
        "\t\tprint('pred: ', pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e9SdJR2Dbyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exctract_disc_patches(images, ground_truth):\n",
        "\t\t\"\"\" Exctract patches with zero distance from otic disc. \"\"\"\n",
        "\t\tpatches = []\n",
        "\t\timg_h, img_w = images[0].shape\n",
        "\t\tfor i, img in enumerate(images):\n",
        "\n",
        "\t\t\tcenter_x = int(ground_truth[i,1])\n",
        "\t\t\tcenter_y = int(ground_truth[i,0])\n",
        "\n",
        "\t\t\thalf_win_h = 22\n",
        "\t\t\thalf_win_w = 21\n",
        "\n",
        "\t\t\tx1 = center_x - half_win_w\n",
        "\t\t\ty1 = center_y - half_win_h\n",
        "\n",
        "\t\t\tif x1 < 0:\n",
        "\t\t\t\tx1 = 0\n",
        "\t\t\tif y1 < 0:\n",
        "\t\t\t\ty1 = 0\n",
        "\t\t\tif (x1 + 42) >= img_w:\n",
        "\t\t\t\tx1 = img_w - 42\n",
        "\t\t\tif (y1 + 45) >= img_h:\n",
        "\t\t\t\ty1 = img_h - 45\n",
        "\n",
        "\t\t\tx2 = x1 + 42\n",
        "\t\t\ty2 = y1 + 45\n",
        "\n",
        "\t\t\tpatch = img[y1:y2, x1:x2]\n",
        "\n",
        "\t\t\tpatches.append(patch)\n",
        "\t\t\t#patches.append(ndimage.rotate(patch, 180))\n",
        "\n",
        "\t\treturn np.array(patches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAHAfUmnDb07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(images_folder, gt_file):\n",
        "\t\timages = []\n",
        "\t\tfor filename in os.listdir(images_folder):\n",
        "\t\t\timg = imread(os.path.join(images_folder, filename), mode='L')\n",
        "\t\t\tif img is not None:\n",
        "\t\t\t\timages.append(imresize(img, (201, 233)))\n",
        "\t\ttrain_images = np.array(images)\n",
        "\n",
        "\t\ttrain_output = np.loadtxt(gt_file)\n",
        "      \n",
        "#img='data/me'\n",
        "\t\t# delete images without annotations (values (-1, -1))\n",
        "\t\t#indices_to_delete = ~(train_output==-1).any(1)\n",
        "\t\t#train_images = train_images[indices_to_delete]\n",
        "\t\t#train_output = train_output[indices_to_delete]\n",
        "\n",
        "\t\t# resize output\n",
        "\t\ttrain_output = np.rint(train_output / 3)\n",
        "\n",
        "\t\treturn train_images, train_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iedhmnMYDb3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_patches( images, gt):\n",
        "\t\ttrain_patches = []\n",
        "\t\tpatch_dists = []\t# ground truth distance of a patch from the optic disc\n",
        "\t\tprint(\"Creating patches...\")\n",
        "\t\tfor img_id, img in enumerate(images):\n",
        "\n",
        "\t\t\t# the less overlap allowed, the more patches created\n",
        "\t\t\tpatches = extract_patches(img, (45, 42), overlap_allowed=0.2, cropvalue=None, crop_fraction_allowed=0.1)\n",
        "\t\t\ttrain_patches.extend(patches)\n",
        "\n",
        "\t\t\tfor patch_id, patch in enumerate(patches):\n",
        "\t\t\t\tpatch_x2d = patch_id % 24 # patches in a row\n",
        "\t\t\t\tpatch_y2d = patch_id / 24 # patches in a column              \n",
        "\t\t\t\tmid_x = patch_x2d * 8 + 21 # column step + width/2\n",
        "\t\t\t\tmid_y = patch_y2d * 9 + 23 # row step + height/2\n",
        "\n",
        "\t\t\t\tx_offset = gt[img_id, 1] - mid_x\n",
        "\t\t\t\ty_offset = gt[img_id, 0] - mid_y\n",
        "\n",
        "\t\t\t\toffset = np.array([y_offset, x_offset])\n",
        "\t\t\t\tpatch_dists.append(offset)\n",
        "\n",
        "\t\treturn np.array(train_patches), np.array(patch_dists)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weSlG7KgEE1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle(images, gt):\n",
        "\t\tshuffle = list(zip(images, gt))\n",
        "\t\trandom.shuffle(shuffle)\n",
        "\t\timages, gt = zip(*shuffle)\n",
        "\n",
        "\t\treturn np.array(images), np.array(gt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwtx_YsAEFCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_test_data():\n",
        "\t\timages_folder = './images/test/'\n",
        "\t\tground_truth = './images/gt_test.txt'\n",
        "\n",
        "\t\ttrain_images, train_output = load_data(images_folder, ground_truth)\n",
        "\t\ttrain_images = preprocess(train_images)\n",
        "\n",
        "\t\tdisc_patches = exctract_disc_patches(train_images, train_output)\n",
        "\t\tzero_dists = np.zeros((len(disc_patches), 2))\n",
        "\t\tprint('Disc patches: ', np.shape(disc_patches))\n",
        "\n",
        "\t\ttrain_images, train_output = make_patches(train_images, train_output)\n",
        "\t\tprint('Orig patches: ', np.shape(train_images))\n",
        "\n",
        "\t\ttrain_output = np.concatenate((train_output, zero_dists), axis=0)\n",
        "\t\ttrain_images = np.concatenate((train_images, disc_patches), axis=0)\n",
        "\n",
        "\t\t# randomly shuffle train array\n",
        "\t\ttrain_images, train_output = shuffle(train_images, train_output)\n",
        "\n",
        "\t\tprint(\"Train samples:\", np.shape(train_images))\n",
        "\t\tprint(\"Output samples:\", np.shape(train_output))\n",
        "\n",
        "\t\t# create tensors\n",
        "\t\ttrain_output = train_output[:, np.newaxis, np.newaxis, : ]\n",
        "\t\ttrain_images = train_images[..., np.newaxis]\n",
        "\n",
        "\t\treturn train_images, train_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30_r9jdREFGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "beb53591-3047-430a-b3f7-de21938d64b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chevFeeXEU8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_folder='/content/drive/My Drive/DRIONS-DB/images'\n",
        "ground_truth='/content/drive/My Drive/DRIONS-DB/experts_anotation/anotExpert1_001.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKRNQ29XE9XR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "72a1e752-cd84-466e-81ad-42e8f6d362f9"
      },
      "source": [
        "train_images, train_output = load_data(images_folder, ground_truth)\n",
        "train_images = preprocess(train_images)\n",
        "\n",
        "disc_patches = exctract_disc_patches(train_images, train_output)\n",
        "zero_dists = np.zeros((len(disc_patches), 2))\n",
        "print('Disc patches: ', np.shape(disc_patches))\n",
        "\n",
        "train_images, train_output = make_patches(train_images, train_output)\n",
        "print('Orig patches: ', np.shape(train_images))\n",
        "\n",
        "train_output = np.concatenate((train_output, zero_dists), axis=0)\n",
        "train_images = np.concatenate((train_images, disc_patches), axis=0)\n",
        "\n",
        "\t\t# randomly shuffle train array\n",
        "train_images, train_output = shuffle(train_images, train_output)\n",
        "\n",
        "print(\"Train samples:\", np.shape(train_images))\n",
        "print(\"Output samples:\", np.shape(train_output))\n",
        "\n",
        "\t\t# create tensors\n",
        "train_output = train_output[:, np.newaxis, np.newaxis, : ]\n",
        "train_images = train_images[..., np.newaxis]\n",
        "\n",
        "train_set = train_images\n",
        "train_y = train_output\n",
        "print(\"train_y: \", np.shape(train_y))\n",
        "print(\"train_set: \", np.shape(train_set))\n",
        "\n",
        "test_images, test_output = prepare_test_data()\n",
        "test_set = test_images[:130, ...]\n",
        "test_y = test_output[:130, ...]\n",
        "print(\"test_y: \", np.shape(test_y))\n",
        "print(\"test_set: \", np.shape(test_set))\n",
        "\n",
        "y = tf.placeholder(\"float32\", [None, 1, 1, 2])\n",
        "\n",
        "cost = tf.reduce_mean(tf.losses.mean_squared_error(labels=y, predictions=self.output))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "training_epochs = 150\n",
        "display_step = 4\n",
        "batch_size = 130\n",
        "with sess as sess:           \n",
        "\tprint('train(): Training started')\n",
        "\tsess.run(tf.global_variables_initializer())\n",
        "\n",
        "\tfor epoch in range(training_epochs):\n",
        "\n",
        "\t\tavg_cost = 0.0\n",
        "\t\ttotal_batch = int(len(train_set) / batch_size) \n",
        "\t\tx_batches = np.array_split(train_set, total_batch)\n",
        "\t\ty_batches = np.array_split(train_y, total_batch)\n",
        "\n",
        "\t\tfor i in range(total_batch):\n",
        "\n",
        "\t\t\tbatch_x, batch_y = x_batches[i], y_batches[i]\n",
        "\n",
        "\t\t\t_, c = sess.run([optimizer, cost], \n",
        "\t\t\t\t\t\t\tfeed_dict={\n",
        "\t\t\t\t\t\t\t\timgs: batch_x, \n",
        "\t\t\t\t\t\t\t\ty: batch_y, \n",
        "\t\t\t\t\t\t\t\ttrain_mode: True\n",
        "\t\t\t\t\t\t\t})\n",
        "\t\t\tavg_cost += c / total_batch\n",
        "\n",
        "\t\tif epoch % display_step == 0:\n",
        "\t\t\tprint(\"\\nEpoch:\", '%04d' % (epoch+1), \"\\nmse(train_set)=\", \\\n",
        "\"{:.9f}\".format(avg_cost))\n",
        "\n",
        "\t\t\tpred_y = sess.run(output, \n",
        "\t\t\t\t\t\t\t\tfeed_dict={\n",
        "\t\t\t\t\t\t\t\t\timgs: test_set,\n",
        "\t\t\t\t\t\t\t\t\t\ttrain_mode: False\n",
        "\t\t\t\t\t\t\t\t\t})\n",
        "\t\t\tmse = tf.reduce_mean(tf.square(pred_y - test_y))\n",
        "\t\t\tprint(\"MSE(test_set): %.4f\" % sess.run(mse)) \n",
        "\n",
        "\t\t\teval_test_img(sess, 90, test_set[90], test_y[90])\n",
        "\t\t\teval_test_img(sess, 100, test_set[100], test_y[100])\n",
        "\t\t\teval_test_img(sess, 20, test_set[20], test_y[20])\n",
        "\n",
        "\tsaver.save(sess, './model')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-276c71a67c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdisc_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexctract_disc_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mzero_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Disc patches: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6b815d027b3c>\u001b[0m in \u001b[0;36mexctract_disc_patches\u001b[0;34m(images, ground_truth)\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                         \u001b[0mcenter_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                         \u001b[0mcenter_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 36 is out of bounds for axis 0 with size 36"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5LDMX2rE9Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}