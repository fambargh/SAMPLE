{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fambargh/SAMPLE/blob/master/Untitled22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72XNv0mvqhRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3acb886b-4264-4fb8-a517-ea69533dafd9"
      },
      "source": [
        "pip install Pillow\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K5rLOpUqjvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "70f31cd8-be8f-4d1a-b6d5-75dce8ae7009"
      },
      "source": [
        "pip install scipy==1.1.0\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.16.4)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.3.0\n",
            "    Uninstalling scipy-1.3.0:\n",
            "      Successfully uninstalled scipy-1.3.0\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wIr1Y_aqjx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.misc import imread, imresize, imsave\n",
        "import time\n",
        "from scipy import misc, ndimage\n",
        "import random\n",
        "from scipy.signal import medfilt\n",
        "import cv2\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GcFK2CWqj0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgs = tf.placeholder(tf.float32, [None, 45, 42, 1])\n",
        "train_mode = tf.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nloEhSUqkAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convlayers():\n",
        "\n",
        "\t\tprint('\\nconvlayers(): Initializing layers')\n",
        "\n",
        "\t\tparameters = []\n",
        "\n",
        "\t\t# normalize input\n",
        "\t\twith tf.name_scope('preprocess') as scope:\n",
        "\t\t\timages = imgs/255 - 0.5\n",
        "\n",
        "\t\twith tf.name_scope('conv1_1') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 1, 8], name='weights1_1')\n",
        "\t\t\tconv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[8], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tconv1_1 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\tpool1 = tf.nn.max_pool(conv1_1,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool1')\n",
        "\n",
        "\t\tdropout1 = tf.layers.dropout(pool1,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=train_mode,\n",
        "\t\t\t                         name='dropout1')\n",
        "\n",
        "\t\twith tf.name_scope('conv1_2') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 8, 16], name='weights1_2')\n",
        "\t\t\tconv = tf.nn.conv2d(dropout1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[16], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tconv1_2 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\tpool2 = tf.nn.max_pool(conv1_2,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool2')\n",
        "\n",
        "    \n",
        "\t\tdropout2 = tf.layers.dropout(pool2,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=train_mode,\n",
        "\t\t\t                         name='dropout2')\n",
        "    \n",
        "\t\twith tf.name_scope('conv1_3') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 16, 32], name='weights1_3')\n",
        "\t\t\tconv = tf.nn.conv2d(dropout2, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[32], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tconv1_3 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\tpool3 = tf.nn.max_pool(conv1_3,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool3')\n",
        "\n",
        "\t\tdropout3 = tf.layers.dropout(pool3,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=train_mode,\n",
        "\t\t\t                         name='dropout3')\n",
        "\n",
        "\t\twith tf.name_scope('conv1_4') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[1, 1, 32, 16], name='weights1_4')\n",
        "\t\t\tconv = tf.nn.conv2d(dropout3, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[16], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tconv1_4 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\twith tf.name_scope('conv1_6') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[1, 1, 16, 2], name='weights1_6')\n",
        "\t\t\tconv = tf.nn.conv2d(conv1_4, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[2], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\toutput = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tparameters += [kernel, biases]\n",
        "\n",
        "\t\tsaver = tf.train.Saver({'W1': parameters[0], 'b1': parameters[1], \n",
        "\t\t\t                         'W2': parameters[2], 'b2': parameters[3], \n",
        "\t\t\t                         'W3': parameters[4], 'b3': parameters[5], \n",
        "\t\t\t                         'W4': parameters[6], 'b4': parameters[7], \n",
        "\t\t\t                         'W5': parameters[8], 'b5': parameters[9]})\n",
        "\n",
        "\t\tprint('output: ', np.shape(output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGSsGWGHFbYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_patches( image, patchshape, overlap_allowed=0.5, cropvalue=None,\n",
        "\t\t\t\t\tcrop_fraction_allowed=0.1):\n",
        "\t\t\"\"\"\n",
        "\t\tGiven an image, extract patches of a given shape with a certain\n",
        "\t\tamount of allowed overlap between patches, using a heuristic to\n",
        "\t\tensure maximum coverage.\n",
        "\t\tIf cropvalue is specified, it is treated as a flag denoting a pixel\n",
        "\t\tthat has been cropped. Patch will be rejected if it has more than\n",
        "\t\tcrop_fraction_allowed * prod(patchshape) pixels equal to cropvalue.\n",
        "\t\tLikewise, patches will be rejected for having more overlap_allowed\n",
        "\t\tfraction of their pixels contained in a patch already selected.\n",
        "\t\t\"\"\"\n",
        "\t\tjump_cols = int(patchshape[1] * overlap_allowed)\n",
        "\t\tjump_rows = int(patchshape[0] * overlap_allowed)\n",
        "\t\t\n",
        "\t\t# Restrict ourselves to the rectangle containing non-cropped pixels\n",
        "\t\tif cropvalue is not None:\n",
        "\t\t\trows, cols = np.where(image != cropvalue)\n",
        "\t\t\trows.sort(); cols.sort()\n",
        "\t\t\tactive =  image[rows[0]:rows[-1], cols[0]:cols[-1]]\n",
        "\t\telse:\n",
        "\t\t\tactive = image\n",
        "\n",
        "\t\trowstart = 0; colstart = 0\n",
        "\n",
        "\t\t# Array tracking where we've already taken patches.\n",
        "\t\tcovered = np.zeros(active.shape, dtype=bool)\n",
        "\t\tpatches = []\n",
        "\n",
        "\t\twhile rowstart < active.shape[0] - patchshape[0]:\n",
        "\t\t\t# Record whether or not e've found a patch in this row, \n",
        "\t\t\t# so we know whether to skip ahead.\n",
        "\t\t\tgot_a_patch_this_row = False\n",
        "\t\t\tcolstart = 0\n",
        "\t\t\twhile colstart < active.shape[1] - patchshape[1]:\n",
        "\t\t\t\t# Slice tuple indexing the region of our proposed patch\n",
        "\t\t\t\tregion = (slice(rowstart, rowstart + patchshape[0]),\n",
        "\t\t\t\t\t\t  slice(colstart, colstart + patchshape[1]))\n",
        "\t\t\t\t\n",
        "\t\t\t\t# The actual pixels in that region.\n",
        "\t\t\t\tpatch = active[region]\n",
        "\n",
        "\t\t\t\t# The current mask value for that region.\n",
        "\t\t\t\tcover_p = covered[region]\n",
        "\t\t\t\tif cropvalue is None or \\\n",
        "\t\t\t\t   frac_eq_to(patch, cropvalue) <= crop_fraction_allowed and \\\n",
        "\t\t\t\t   frac_eq_to(cover_p, True) <= overlap_allowed:\n",
        "\t\t\t\t\t# Accept the patch.\n",
        "\t\t\t\t\tpatches.append(patch)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Mask the area.\n",
        "\t\t\t\t\tcovered[region] = True\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Jump ahead in the x direction.\n",
        "\t\t\t\t\tcolstart += jump_cols\n",
        "\t\t\t\t\tgot_a_patch_this_row = True\n",
        "\t\t\t\t\t#print \"Got a patch at %d, %d\" % (rowstart, colstart)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# Otherwise, shift window across by one pixel.\n",
        "\t\t\t\t\tcolstart += 1\n",
        "\n",
        "\t\t\tif got_a_patch_this_row:\n",
        "\t\t\t\t# Jump ahead in the y direction.\n",
        "\t\t\t\trowstart += jump_rows\n",
        "\t\t\telse:\n",
        "\t\t\t\t# Otherwise, shift the window down by one pixel.\n",
        "\t\t\t\trowstart += 1\n",
        "\n",
        "\t\t\t# Return a 3D array of the patches with the patch index as the first\n",
        "\t\t\t# dimension (so that patch pixels stay contiguous in memory, in a \n",
        "\t\t\t# C-ordered array).\n",
        "\n",
        "\t\treturn np.concatenate([pat[np.newaxis, ...] for pat in patches], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu_XVWmGFbd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess( images):\n",
        "\n",
        "\t\tnew_images = []\n",
        "\n",
        "\t\tfor i, img in enumerate(images):\n",
        "\n",
        "\t\t\tclahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(40,40))\n",
        "\t\t\timg = clahe.apply(img)\n",
        "\n",
        "\t\t\timg = cv2.bilateralFilter(img, -1, 20, 20)\n",
        "\n",
        "\t\t\tintensities = medfilt(img, (21, 21))\n",
        "\t\t\tintensities = intensities.astype(np.float32)\n",
        "\t\t\tintensities_smoothed = cv2.bilateralFilter(intensities, -1, 70, 13)\n",
        "\t\t\twidth, height = img.shape\n",
        "\t\t\timg[0:width, 0:height] = img[0:width, 0:height] + (90) - intensities_smoothed[0:width, 0:height]\n",
        "\t\t\tidx = img[:] > 210\n",
        "\t\t\timg[idx] = 18\n",
        "\n",
        "\t\t\tclahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(1,1))\n",
        "\t\t\timg = clahe.apply(img)\n",
        "\n",
        "\t\t\tnew_images.append(img)\n",
        "\n",
        "\t\treturn np.array(new_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_erOQTzFmBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_test_img( sess, idx, img, gt):\n",
        "\n",
        "\t\ttest = img[np.newaxis, ... ]\n",
        "\n",
        "\t\tpred = sess.run(output, feed_dict={\n",
        "\t\t\t\t\t\t\timgs: test, \n",
        "\t\t\t\t\t\t\ttrain_mode: False\n",
        "\t\t\t\t\t\t})\n",
        "\t\tprint('TEST_set[', idx, ']:')\n",
        "\t\tprint('gr_t: ', gt)\n",
        "\t\tprint('pred: ', pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rovf_oP4FmJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exctract_disc_patches(images, ground_truth):\n",
        "\t\t\"\"\" Exctract patches with zero distance from otic disc. \"\"\"\n",
        "\t\tpatches = []\n",
        "\t\timg_h, img_w = images[0].shape\n",
        "\t\tfor i, img in enumerate(images):\n",
        "\n",
        "\t\t\tcenter_x = int(ground_truth[i,1])\n",
        "\t\t\tcenter_y = int(ground_truth[i,0])\n",
        "\n",
        "\t\t\thalf_win_h = 22\n",
        "\t\t\thalf_win_w = 21\n",
        "\n",
        "\t\t\tx1 = center_x - half_win_w\n",
        "\t\t\ty1 = center_y - half_win_h\n",
        "\n",
        "\t\t\tif x1 < 0:\n",
        "\t\t\t\tx1 = 0\n",
        "\t\t\tif y1 < 0:\n",
        "\t\t\t\ty1 = 0\n",
        "\t\t\tif (x1 + 42) >= img_w:\n",
        "\t\t\t\tx1 = img_w - 42\n",
        "\t\t\tif (y1 + 45) >= img_h:\n",
        "\t\t\t\ty1 = img_h - 45\n",
        "\n",
        "\t\t\tx2 = x1 + 42\n",
        "\t\t\ty2 = y1 + 45\n",
        "\n",
        "\t\t\tpatch = img[y1:y2, x1:x2]\n",
        "\n",
        "\t\t\tpatches.append(patch)\n",
        "\t\t\t#patches.append(ndimage.rotate(patch, 180))\n",
        "\n",
        "\t\treturn np.array(patches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLAMiRZ-FmL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(images_folder, gt_file):\n",
        "\t\timages = []\n",
        "\t\tfor filename in os.listdir(images_folder):\n",
        "\t\t\timg = imread(os.path.join(images_folder, filename), mode='L')\n",
        "\t\t\tif img is not None:\n",
        "\t\t\t\timages.append(imresize(img, (201, 233)))\n",
        "\t\ttrain_images = np.array(images)\n",
        "\n",
        "\t\ttrain_output = np.loadtxt(gt_file)\n",
        "      \n",
        "#img='data/me'\n",
        "\t\t# delete images without annotations (values (-1, -1))\n",
        "\t\t#indices_to_delete = ~(train_output==-1).any(1)\n",
        "\t\t#train_images = train_images[indices_to_delete]\n",
        "\t\t#train_output = train_output[indices_to_delete]\n",
        "\n",
        "\t\t# resize output\n",
        "\t\ttrain_output = np.rint(train_output / 3)\n",
        "\n",
        "\t\treturn train_images, train_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riUeiJSOFx5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_patches( images, gt):\n",
        "\t\ttrain_patches = []\n",
        "\t\tpatch_dists = []\t# ground truth distance of a patch from the optic disc\n",
        "\t\tprint(\"Creating patches...\")\n",
        "\t\tfor img_id, img in enumerate(images):\n",
        "\n",
        "\t\t\t# the less overlap allowed, the more patches created\n",
        "\t\t\tpatches = extract_patches(img, (45, 42), overlap_allowed=0.2, cropvalue=None, crop_fraction_allowed=0.1)\n",
        "\t\t\ttrain_patches.extend(patches)\n",
        "\n",
        "\t\t\tfor patch_id, patch in enumerate(patches):\n",
        "\t\t\t\tpatch_x2d = patch_id % 24 # patches in a row\n",
        "\t\t\t\tpatch_y2d = patch_id / 24 # patches in a column              \n",
        "\t\t\t\tmid_x = patch_x2d * 8 + 21 # column step + width/2\n",
        "\t\t\t\tmid_y = patch_y2d * 9 + 23 # row step + height/2\n",
        "\n",
        "\t\t\t\tx_offset = gt[img_id, 1] - mid_x\n",
        "\t\t\t\ty_offset = gt[img_id, 0] - mid_y\n",
        "\n",
        "\t\t\t\toffset = np.array([y_offset, x_offset])\n",
        "\t\t\t\tpatch_dists.append(offset)\n",
        "\n",
        "\t\treturn np.array(train_patches), np.array(patch_dists)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb2svEtJFx-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle(images, gt):\n",
        "\t\tshuffle = list(zip(images, gt))\n",
        "\t\trandom.shuffle(shuffle)\n",
        "\t\timages, gt = zip(*shuffle)\n",
        "\n",
        "\t\treturn np.array(images), np.array(gt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "441jHnVHFyBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_test_data():\n",
        "\t\timages_folder = '/content/drive/My Drive/drions/images'\n",
        "\t\tground_truth = '/content/drive/My Drive/anotExpert1_001.txt'\n",
        "\t\ttrain_images, train_output = load_data(images_folder, ground_truth)\n",
        "\t\ttrain_images = preprocess(train_images)\n",
        "\n",
        "\t\tdisc_patches = exctract_disc_patches(train_images, train_output)\n",
        "\t\tzero_dists = np.zeros((len(disc_patches), 2))\n",
        "\t\tprint('Disc patches: ', np.shape(disc_patches))\n",
        "\n",
        "\t\ttrain_images, train_output = make_patches(train_images, train_output)\n",
        "\t\tprint('Orig patches: ', np.shape(train_images))\n",
        "\n",
        "\t\ttrain_output = np.concatenate((train_output, zero_dists), axis=0)\n",
        "\t\ttrain_images = np.concatenate((train_images, disc_patches), axis=0)\n",
        "\n",
        "\t\t# randomly shuffle train array\n",
        "\t\ttrain_images, train_output = shuffle(train_images, train_output)\n",
        "\n",
        "\t\tprint(\"Train samples:\", np.shape(train_images))\n",
        "\t\tprint(\"Output samples:\", np.shape(train_output))\n",
        "\n",
        "\t\t# create tensors\n",
        "\t\ttrain_output = train_output[:, np.newaxis, np.newaxis, : ]\n",
        "\t\ttrain_images = train_images[..., np.newaxis]\n",
        "\n",
        "\t\treturn train_images, train_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHMvdEj1GAzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b94a5413-a2e5-4393-dca5-907e16b0a0e0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocOrGrPsGA1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_folder='/content/drive/My Drive/drions/images'\n",
        "#for m in range (1,111):   \n",
        "   # ground_truth='/content/drive/My Drive/drions/ground_truth/image_{}.txt'.format(m)\n",
        "ground_truth='/content/drive/My Drive/anotExpert1_001.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc60_4wmxlcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIdkKuIeGA3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "outputId": "f5bde210-49b5-4ceb-e2b1-4e35db3c746c"
      },
      "source": [
        "train_images, train_output = load_data(images_folder, ground_truth)\n",
        "train_images = preprocess(train_images)\n",
        "\n",
        "disc_patches = exctract_disc_patches(train_images, train_output)\n",
        "zero_dists = np.zeros((len(disc_patches), 2))\n",
        "print('Disc patches: ', np.shape(disc_patches))\n",
        "\n",
        "train_images, train_output = make_patches(train_images, train_output)\n",
        "print('Orig patches: ', np.shape(train_images))\n",
        "\n",
        "train_output = np.concatenate((train_output, zero_dists), axis=0)\n",
        "train_images = np.concatenate((train_images, disc_patches), axis=0)\n",
        "\n",
        "\t\t# randomly shuffle train array\n",
        "train_images, train_output = shuffle(train_images, train_output)\n",
        "\n",
        "print(\"Train samples:\", np.shape(train_images))\n",
        "print(\"Output samples:\", np.shape(train_output))\n",
        "\n",
        "\t\t# create tensors\n",
        "train_output = train_output[:, np.newaxis, np.newaxis, : ]\n",
        "train_images = train_images[..., np.newaxis]\n",
        "\n",
        "train_set = train_images\n",
        "train_y = train_output\n",
        "print(\"train_y: \", np.shape(train_y))\n",
        "print(\"train_set: \", np.shape(train_set))\n",
        "\n",
        "test_images, test_output = prepare_test_data()\n",
        "test_set = test_images[:130, ...]\n",
        "test_y = test_output[:130, ...]\n",
        "print(\"test_y: \", np.shape(test_y))\n",
        "print(\"test_set: \", np.shape(test_set))\n",
        "\n",
        "y = tf.placeholder(\"float32\", [None, 1, 1, 2])\n",
        "convlayers()\n",
        "cost = tf.reduce_mean(tf.losses.mean_squared_error(labels=y, predictions=output))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "training_epochs = 150\n",
        "display_step = 4\n",
        "batch_size = 130\n",
        "with sess as sess:           \n",
        "\tprint('train(): Training started')\n",
        "\tsess.run(tf.global_variables_initializer())\n",
        "\n",
        "\tfor epoch in range(training_epochs):\n",
        "\n",
        "\t\tavg_cost = 0.0\n",
        "\t\ttotal_batch = int(len(train_set) / batch_size) \n",
        "\t\tx_batches = np.array_split(train_set, total_batch)\n",
        "\t\ty_batches = np.array_split(train_y, total_batch)\n",
        "\n",
        "\t\tfor i in range(total_batch):\n",
        "\n",
        "\t\t\tbatch_x, batch_y = x_batches[i], y_batches[i]\n",
        "\n",
        "\t\t\t_, c = sess.run([optimizer, cost], \n",
        "\t\t\t\t\t\t\tfeed_dict={\n",
        "\t\t\t\t\t\t\t\timgs: batch_x, \n",
        "\t\t\t\t\t\t\t\ty: batch_y, \n",
        "\t\t\t\t\t\t\t\ttrain_mode: True\n",
        "\t\t\t\t\t\t\t})\n",
        "\t\t\tavg_cost += c / total_batch\n",
        "\n",
        "\t\tif epoch % display_step == 0:\n",
        "\t\t\tprint(\"\\nEpoch:\", '%04d' % (epoch+1), \"\\nmse(train_set)=\", \\\n",
        "\"{:.9f}\".format(avg_cost))\n",
        "\n",
        "\t\t\tpred_y = sess.run(output, \n",
        "\t\t\t\t\t\t\t\tfeed_dict={\n",
        "\t\t\t\t\t\t\t\t\timgs: test_set,\n",
        "\t\t\t\t\t\t\t\t\t\ttrain_mode: False\n",
        "\t\t\t\t\t\t\t\t\t})\n",
        "\t\t\tmse = tf.reduce_mean(tf.square(pred_y - test_y))\n",
        "\t\t\tprint(\"MSE(test_set): %.4f\" % sess.run(mse)) \n",
        "\n",
        "\t\t\teval_test_img(sess, 90, test_set[90], test_y[90])\n",
        "\t\t\teval_test_img(sess, 100, test_set[100], test_y[100])\n",
        "\t\t\teval_test_img(sess, 20, test_set[20], test_y[20])\n",
        "\n",
        "\tsaver.save(sess, './model')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Disc patches:  (110, 45, 42)\n",
            "Creating patches...\n",
            "Orig patches:  (47520, 45, 42)\n",
            "Train samples: (47630, 45, 42)\n",
            "Output samples: (47630, 2)\n",
            "train_y:  (47630, 1, 1, 2)\n",
            "train_set:  (47630, 45, 42, 1)\n",
            "Disc patches:  (110, 45, 42)\n",
            "Creating patches...\n",
            "Orig patches:  (47520, 45, 42)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0810 11:14:46.119100 140418919503744 deprecation.py:323] From <ipython-input-4-79aebf0245ef>:29: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train samples: (47630, 45, 42)\n",
            "Output samples: (47630, 2)\n",
            "test_y:  (130, 1, 1, 2)\n",
            "test_set:  (130, 45, 42, 1)\n",
            "\n",
            "convlayers(): Initializing layers\n",
            "output:  (?, 1, 1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0e3f54987b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mconvlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    }
  ]
}