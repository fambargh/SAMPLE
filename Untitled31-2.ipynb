{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fambargh/SAMPLE/blob/master/Untitled31-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXY-HJXjJQ4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byj0UKdUN73F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "963815e3-0fd0-40f6-dcaf-907df10b00a0"
      },
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import UpSampling2D, Conv2D, Activation, BatchNormalization, Reshape, Dense, Input, LeakyReLU, Dropout, Flatten\n",
        "from keras import optimizers \n",
        "import numpy as np\n",
        "from keras import layers\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "from ast import literal_eval\n",
        "\n",
        "#from scipy.misc import imsave,imresize\n",
        "from keras import backend"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szuzu4wMJT00",
        "colab_type": "code",
        "outputId": "56748312-a675-4069-a6f3-906651fc6f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soGwvGQ2JT3F",
        "colab_type": "code",
        "outputId": "aba6f8d5-9868-4498-a16c-a497b3c2bb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "image_path='/content/drive/My Drive/muip'\n",
        "#r=os.path.join(image_path,'sanas')\n",
        "#os.mkdir(image_path)\n",
        "t_d=ImageDataGenerator(rescale=1./255)\n",
        "t_g=t_d.flow_from_directory(image_path,target_size=(128,128),batch_size=20)\n",
        "#print(image_path.size)\n",
        "#im=image.load_img(image_path)\n",
        "#print(im.format)\n",
        "#from keras import cifar10\n",
        "#c=cifar10.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP-kfQzgJdGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_directory='./content/drive/My Drive/rigam'\n",
        "img_size=[128,128]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGOGgbkshv1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "from datetime import timedelta\n",
        "import math\n",
        "import os\n",
        "\n",
        "# Use PrettyTensor to simplify Neural Network construction.\n",
        "#import prettytensor as pt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GkUM5bPJdPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator_path='/content/drive/My Drive/rigam'\n",
        "generator_path='/content/drive/My Drive/rigam'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_nqkZ_MJpTI",
        "colab_type": "code",
        "outputId": "840dbeca-c293-4714-b859-f30eb96d9817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        }
      },
      "source": [
        "noise_shape = (100,)\n",
        "kernel_size=5\n",
        "channels=3\n",
        "        # This block of code can be a little daunting, but essentially it automatically calculates the required starting\n",
        "        # array size that will be correctly upscaled to our desired image size.\n",
        "        #\n",
        "        # We have 5 Upsample2D layers which each double the images width and height, so we can determine the starting\n",
        "        # x size by taking (x / 2^upsample_count) So for our target image size, 256x192, we do the following:\n",
        "        # x = (192 / 2^5), y = (256 / 2^5) [x and y are reversed within the model]\n",
        "        # We also need a 3rd dimension which is chosen relatively arbitrarily, in this case it's 64.\n",
        "generator_input=Input(shape=noise_shape)\n",
        "x=layers.Dense(128*32*32)(generator_input)\n",
        "x=layers.ReLU()(x)\n",
        "x=layers.Reshape((32,32,128))(x)\n",
        "x=layers.BatchNormalization(momentum=0.8)(x)\n",
        "x=layers.UpSampling2D()(x)\n",
        "x=layers.Conv2D(512, kernel_size=kernel_size,strides=1, padding=\"same\")(x)\n",
        "x=layers.ReLU()(x)\n",
        "x=layers.BatchNormalization(momentum=0.8)(x)\n",
        "x=layers.UpSampling2D()(x)\n",
        "x=layers.Conv2D(256, kernel_size=kernel_size,strides=1, padding=\"same\")(x)\n",
        "x=layers.ReLU()(x)\n",
        "x=layers.BatchNormalization(momentum=0.8)(x)\n",
        "x=layers.Conv2D(128, kernel_size=kernel_size,strides=1, padding=\"same\")(x)\n",
        "x=layers.ReLU()(x)\n",
        "x=layers.BatchNormalization(momentum=0.8)(x)\n",
        "x=layers.Conv2D(channels, kernel_size=kernel_size,strides=1,activation='tanh', padding=\"same\")(x)\n",
        "generator=Model(generator_input,x)\n",
        "generator.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0823 16:59:09.839188 140204950943616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0823 16:59:09.882583 140204950943616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0823 16:59:09.889968 140204950943616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0823 16:59:09.928417 140204950943616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0823 16:59:09.929531 140204950943616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0823 16:59:12.754198 140204950943616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0823 16:59:12.834883 140204950943616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 131072)            13238272  \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 64, 64, 512)       1638912   \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 64, 64, 512)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64, 64, 512)       2048      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 128, 128, 512)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 128, 128, 256)     3277056   \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 128, 128, 256)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128, 128, 256)     1024      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 128, 128, 128)     819328    \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, 128, 128, 128)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128, 128, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 128, 128, 3)       9603      \n",
            "=================================================================\n",
            "Total params: 18,987,267\n",
            "Trainable params: 18,985,219\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoto5eR1JpaZ",
        "colab_type": "code",
        "outputId": "a85a6e5a-00a0-4637-94f2-9d4c0d4380f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "discriminator_input = Input(shape=(img_size[0], img_size[1], channels))\n",
        "x=layers.Conv2D(128, kernel_size=kernel_size, strides=2, padding=\"same\")(discriminator_input)\n",
        "x=layers.LeakyReLU(alpha=0.2)(x)\n",
        "x=layers.Dropout(0.4)(x)\n",
        "x=layers.Conv2D(256, kernel_size=kernel_size, strides=2, padding=\"same\")(x)\n",
        "x=layers.LeakyReLU(alpha=0.2)(x)\n",
        "x=layers.Dropout(0.4)(x)\n",
        "x=layers.Conv2D(512, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n",
        "x=layers.LeakyReLU(alpha=0.2)(x)\n",
        "x=layers.Dropout(0.4)(x)\n",
        "x=layers.Flatten()(x)\n",
        "x=layers.Dense(1, activation='sigmoid')(x)\n",
        "discriminator=Model(discriminator_input,x)\n",
        "discriminator.summary()\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0823 16:59:20.583961 140204950943616 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 64, 64, 128)       9728      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 256)       819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 512)       3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 512)       0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32, 32, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 524288)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 524289    \n",
            "=================================================================\n",
            "Total params: 4,630,785\n",
            "Trainable params: 4,630,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pMIyKQJJpkW",
        "colab_type": "code",
        "outputId": "72a21ae7-94ae-4d79-f180-529fedea0106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizers.SGD(lr=0.01),\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "generator.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(0.0002, 0.5))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0823 16:59:29.273115 140204950943616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0823 16:59:29.282895 140204950943616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0vnaZeTJpmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator_trainable=False\n",
        "gan_input=Input(shape=noise_shape)\n",
        "gan_output=discriminator(generator(gan_input))\n",
        "gan=Model(gan_input,gan_output)\n",
        "gan.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(0.0002, 0.5))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpUPUqthWPrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "x_train=t_g\n",
        "#x_train=image.load_img(image_path)\n",
        "#x_train=x_train.reshape((x_train.shape[0],)+(32,32,128)).astype('float32')/255.\n",
        "iterations=1\n",
        "batch_size=20\n",
        "save_dir=output_directory\n",
        "start=0\n",
        "for step in range(iterations):\n",
        "  r_l_v=np.random.normal(size=(batch_size,100))\n",
        "  \n",
        "  generated_images=generator.predict(r_l_v)\n",
        "  \n",
        "  stop=start+batch_size\n",
        "  real_images=x_train\n",
        "  combined_images=np.concatenate([generated_images,real_images])\n",
        "\n",
        "  labels = np.concatenate([np.ones((batch_size, 1)),np.zeros((batch_size, 1))])\n",
        "  labels += 0.05 * np.random.random(labels.shape)\n",
        "  d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "  random_latent_vectors = np.random.normal(size=(batch_size,100))\n",
        "  misleading_targets = np.zeros((batch_size, 1))\n",
        "  a_loss = gan.train_on_batch(random_latent_vectors,misleading_targets)\n",
        "  start += batch_size\n",
        "  if start > len(x_train) - batch_size:\n",
        "    start = 0\n",
        "    \n",
        "  if step % 100 == 0:\n",
        "    gan.save_weights('gan.h5')\n",
        "    print('discriminator loss:', d_loss)\n",
        "    print('adversarial loss:', a_loss)\n",
        "    img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
        "    img.save(os.path.join(save_dir,'generated_frog' + str(step) + '.jpg'))\n",
        "    img = image.array_to_img(real_images[0] * 255., scale=False)\n",
        "    img.save(os.path.join(save_dir,'real_frog' + str(step) + '.jpg'))\n",
        "    !cp -r models_* gdrive/My\\ Drive/save_dir.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9j9MMgixK-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3fefd87b-3cc8-4dcf-963f-ed0b67296789"
      },
      "source": [
        "!cp -r gdrive/My\\ Drive/rigam/ri/image1-5.jpg."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: missing destination file operand after 'gdrive/My Drive/rigam/ri/image1-5.jpg.'\n",
            "Try 'cp --help' for more information.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7D2IYBL_mQ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5671eb0d-abfa-4940-bee0-1b236895baab"
      },
      "source": [
        "cp --help"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: cp [OPTION]... [-T] SOURCE DEST\n",
            "  or:  cp [OPTION]... SOURCE... DIRECTORY\n",
            "  or:  cp [OPTION]... -t DIRECTORY SOURCE...\n",
            "Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.\n",
            "\n",
            "Mandatory arguments to long options are mandatory for short options too.\n",
            "  -a, --archive                same as -dR --preserve=all\n",
            "      --attributes-only        don't copy the file data, just the attributes\n",
            "      --backup[=CONTROL]       make a backup of each existing destination file\n",
            "  -b                           like --backup but does not accept an argument\n",
            "      --copy-contents          copy contents of special files when recursive\n",
            "  -d                           same as --no-dereference --preserve=links\n",
            "  -f, --force                  if an existing destination file cannot be\n",
            "                                 opened, remove it and try again (this option\n",
            "                                 is ignored when the -n option is also used)\n",
            "  -i, --interactive            prompt before overwrite (overrides a previous -n\n",
            "                                  option)\n",
            "  -H                           follow command-line symbolic links in SOURCE\n",
            "  -l, --link                   hard link files instead of copying\n",
            "  -L, --dereference            always follow symbolic links in SOURCE\n",
            "  -n, --no-clobber             do not overwrite an existing file (overrides\n",
            "                                 a previous -i option)\n",
            "  -P, --no-dereference         never follow symbolic links in SOURCE\n",
            "  -p                           same as --preserve=mode,ownership,timestamps\n",
            "      --preserve[=ATTR_LIST]   preserve the specified attributes (default:\n",
            "                                 mode,ownership,timestamps), if possible\n",
            "                                 additional attributes: context, links, xattr,\n",
            "                                 all\n",
            "      --no-preserve=ATTR_LIST  don't preserve the specified attributes\n",
            "      --parents                use full source file name under DIRECTORY\n",
            "  -R, -r, --recursive          copy directories recursively\n",
            "      --reflink[=WHEN]         control clone/CoW copies. See below\n",
            "      --remove-destination     remove each existing destination file before\n",
            "                                 attempting to open it (contrast with --force)\n",
            "      --sparse=WHEN            control creation of sparse files. See below\n",
            "      --strip-trailing-slashes  remove any trailing slashes from each SOURCE\n",
            "                                 argument\n",
            "  -s, --symbolic-link          make symbolic links instead of copying\n",
            "  -S, --suffix=SUFFIX          override the usual backup suffix\n",
            "  -t, --target-directory=DIRECTORY  copy all SOURCE arguments into DIRECTORY\n",
            "  -T, --no-target-directory    treat DEST as a normal file\n",
            "  -u, --update                 copy only when the SOURCE file is newer\n",
            "                                 than the destination file or when the\n",
            "                                 destination file is missing\n",
            "  -v, --verbose                explain what is being done\n",
            "  -x, --one-file-system        stay on this file system\n",
            "  -Z                           set SELinux security context of destination\n",
            "                                 file to default type\n",
            "      --context[=CTX]          like -Z, or if CTX is specified then set the\n",
            "                                 SELinux or SMACK security context to CTX\n",
            "      --help     display this help and exit\n",
            "      --version  output version information and exit\n",
            "\n",
            "By default, sparse SOURCE files are detected by a crude heuristic and the\n",
            "corresponding DEST file is made sparse as well.  That is the behavior\n",
            "selected by --sparse=auto.  Specify --sparse=always to create a sparse DEST\n",
            "file whenever the SOURCE file contains a long enough sequence of zero bytes.\n",
            "Use --sparse=never to inhibit creation of sparse files.\n",
            "\n",
            "When --reflink[=always] is specified, perform a lightweight copy, where the\n",
            "data blocks are copied only when modified.  If this is not possible the copy\n",
            "fails, or if --reflink=auto is specified, fall back to a standard copy.\n",
            "\n",
            "The backup suffix is '~', unless set with --suffix or SIMPLE_BACKUP_SUFFIX.\n",
            "The version control method may be selected via the --backup option or through\n",
            "the VERSION_CONTROL environment variable.  Here are the values:\n",
            "\n",
            "  none, off       never make backups (even if --backup is given)\n",
            "  numbered, t     make numbered backups\n",
            "  existing, nil   numbered if numbered backups exist, simple otherwise\n",
            "  simple, never   always make simple backups\n",
            "\n",
            "As a special case, cp makes a backup of SOURCE when the force and backup\n",
            "options are given and SOURCE and DEST are the same name for an existing,\n",
            "regular file.\n",
            "\n",
            "GNU coreutils online help: <http://www.gnu.org/software/coreutils/>\n",
            "Full documentation at: <http://www.gnu.org/software/coreutils/cp>\n",
            "or available locally via: info '(coreutils) cp invocation'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}