{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fambargh/SAMPLE/blob/master/odd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDZ3wJC0ue61",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8b4294db-6db0-4743-b2bc-1615c882524d"
      },
      "source": [
        "pip install Pillow\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4FXw6yyukoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cae336b8-624f-4cba-fbeb-ec795423eac2"
      },
      "source": [
        "pip install scipy==1.1.0\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOjrNQ5bu3ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import imread\n",
        "from scipy.misc import imresize, imsave\n",
        "import time\n",
        "from scipy import misc, ndimage\n",
        "import random\n",
        "from scipy.signal import medfilt\n",
        "import cv2\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVQxS-Q-vBDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class optic_disc_detector:\n",
        "\n",
        "\tdef __init__(self, sess=None):\n",
        "\t\tself.imgs = tf.placeholder(tf.float32, [None, 45, 42, 1])\n",
        "\t\tself.train_mode = tf.placeholder(tf.bool)\n",
        "\t\tself.convlayers()\n",
        "\t\tself.output = self.conv1_6\n",
        "\t\tself.sess = sess\n",
        "\n",
        "\tdef convlayers(self):\n",
        "\n",
        "\t\tprint('\\nconvlayers(): Initializing layers')\n",
        "\n",
        "\t\tself.parameters = []\n",
        "\n",
        "\t\t# normalize input\n",
        "\t\twith tf.name_scope('preprocess') as scope:\n",
        "\t\t\timages = self.imgs/255 - 0.5\n",
        "\n",
        "\t\twith tf.name_scope('conv1_1') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 1, 8], name='weights1_1')\n",
        "\t\t\tconv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[8], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tself.conv1_1 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tself.parameters += [kernel, biases]\n",
        "\n",
        "\t\tself.pool1 = tf.nn.max_pool(self.conv1_1,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool1')\n",
        "\n",
        "\t\tself.dropout1 = tf.layers.dropout(self.pool1,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=self.train_mode,\n",
        "\t\t\t                         name='dropout1')\n",
        "\n",
        "\t\twith tf.name_scope('conv1_2') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 8, 16], name='weights1_2')\n",
        "\t\t\tconv = tf.nn.conv2d(self.dropout1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[16], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tself.conv1_2 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tself.parameters += [kernel, biases]\n",
        "\n",
        "\t\tself.pool2 = tf.nn.max_pool(self.conv1_2,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool2')\n",
        "\n",
        "\t\tself.dropout2 = tf.layers.dropout(self.pool2,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=self.train_mode,\n",
        "\t\t\t                         name='dropout2')\n",
        "\n",
        "\t\twith tf.name_scope('conv1_3') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 16, 32], name='weights1_3')\n",
        "\t\t\tconv = tf.nn.conv2d(self.dropout2, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[32], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tself.conv1_3 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tself.parameters += [kernel, biases]\n",
        "\n",
        "\t\tself.pool3 = tf.nn.max_pool(self.conv1_3,\n",
        "\t\t\t\t\t\t\t   ksize=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   strides=[1, 3, 3, 1],\n",
        "\t\t\t\t\t\t\t   padding='SAME',\n",
        "\t\t\t\t\t\t\t   name='pool3')\n",
        "\n",
        "\t\tself.dropout3 = tf.layers.dropout(self.pool3,\n",
        "\t\t\t                         rate=0.2,\n",
        "\t\t\t                         training=self.train_mode,\n",
        "\t\t\t                         name='dropout3')\n",
        "\n",
        "\t\twith tf.name_scope('conv1_4') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[1, 1, 32, 16], name='weights1_4')\n",
        "\t\t\tconv = tf.nn.conv2d(self.dropout3, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[16], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tout = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tself.conv1_4 = tf.nn.relu(out, name=scope)\n",
        "\t\t\tself.parameters += [kernel, biases]\n",
        "\n",
        "\t\twith tf.name_scope('conv1_6') as scope:\n",
        "\t\t\tkernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[1, 1, 16, 2], name='weights1_6')\n",
        "\t\t\tconv = tf.nn.conv2d(self.conv1_4, kernel, [1, 1, 1, 1], padding='VALID')\n",
        "\t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[2], dtype=tf.float32),\n",
        "\t\t\t\t\t\t\t\t trainable=True, name='biases')\n",
        "\t\t\tself.conv1_6 = tf.nn.bias_add(conv, biases)\n",
        "\t\t\tself.parameters += [kernel, biases]\n",
        "\n",
        "\t\tself.saver = tf.train.Saver({'W1': self.parameters[0], 'b1': self.parameters[1], \n",
        "\t\t\t                         'W2': self.parameters[2], 'b2': self.parameters[3], \n",
        "\t\t\t                         'W3': self.parameters[4], 'b3': self.parameters[5], \n",
        "\t\t\t                         'W4': self.parameters[6], 'b4': self.parameters[7], \n",
        "\t\t\t                         'W5': self.parameters[8], 'b5': self.parameters[9]})\n",
        "\n",
        "\t\tprint('output: ', np.shape(self.conv1_6))\n",
        "\n",
        "\tdef extract_patches(self, image, patchshape, overlap_allowed=0.5, cropvalue=None,\n",
        "\t\t\t\t\tcrop_fraction_allowed=0.1):\n",
        "\t\t\"\"\"\n",
        "\t\tGiven an image, extract patches of a given shape with a certain\n",
        "\t\tamount of allowed overlap between patches, using a heuristic to\n",
        "\t\tensure maximum coverage.\n",
        "\t\tIf cropvalue is specified, it is treated as a flag denoting a pixel\n",
        "\t\tthat has been cropped. Patch will be rejected if it has more than\n",
        "\t\tcrop_fraction_allowed * prod(patchshape) pixels equal to cropvalue.\n",
        "\t\tLikewise, patches will be rejected for having more overlap_allowed\n",
        "\t\tfraction of their pixels contained in a patch already selected.\n",
        "\t\t\"\"\"\n",
        "\t\tjump_cols = int(patchshape[1] * overlap_allowed)\n",
        "\t\tjump_rows = int(patchshape[0] * overlap_allowed)\n",
        "\t\t\n",
        "\t\t# Restrict ourselves to the rectangle containing non-cropped pixels\n",
        "\t\tif cropvalue is not None:\n",
        "\t\t\trows, cols = np.where(image != cropvalue)\n",
        "\t\t\trows.sort(); cols.sort()\n",
        "\t\t\tactive =  image[rows[0]:rows[-1], cols[0]:cols[-1]]\n",
        "\t\telse:\n",
        "\t\t\tactive = image\n",
        "\n",
        "\t\trowstart = 0; colstart = 0\n",
        "\n",
        "\t\t# Array tracking where we've already taken patches.\n",
        "\t\tcovered = np.zeros(active.shape, dtype=bool)\n",
        "\t\tpatches = []\n",
        "\n",
        "\t\twhile rowstart < active.shape[0] - patchshape[0]:\n",
        "\t\t\t# Record whether or not e've found a patch in this row, \n",
        "\t\t\t# so we know whether to skip ahead.\n",
        "\t\t\tgot_a_patch_this_row = False\n",
        "\t\t\tcolstart = 0\n",
        "\t\t\twhile colstart < active.shape[1] - patchshape[1]:\n",
        "\t\t\t\t# Slice tuple indexing the region of our proposed patch\n",
        "\t\t\t\tregion = (slice(rowstart, rowstart + patchshape[0]),\n",
        "\t\t\t\t\t\t  slice(colstart, colstart + patchshape[1]))\n",
        "\t\t\t\t\n",
        "\t\t\t\t# The actual pixels in that region.\n",
        "\t\t\t\tpatch = active[region]\n",
        "\n",
        "\t\t\t\t# The current mask value for that region.\n",
        "\t\t\t\tcover_p = covered[region]\n",
        "\t\t\t\tif cropvalue is None or \\\n",
        "\t\t\t\t   frac_eq_to(patch, cropvalue) <= crop_fraction_allowed and \\\n",
        "\t\t\t\t   frac_eq_to(cover_p, True) <= overlap_allowed:\n",
        "\t\t\t\t\t# Accept the patch.\n",
        "\t\t\t\t\tpatches.append(patch)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Mask the area.\n",
        "\t\t\t\t\tcovered[region] = True\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Jump ahead in the x direction.\n",
        "\t\t\t\t\tcolstart += jump_cols\n",
        "\t\t\t\t\tgot_a_patch_this_row = True\n",
        "\t\t\t\t\t#print \"Got a patch at %d, %d\" % (rowstart, colstart)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# Otherwise, shift window across by one pixel.\n",
        "\t\t\t\t\tcolstart += 1\n",
        "\n",
        "\t\t\tif got_a_patch_this_row:\n",
        "\t\t\t\t# Jump ahead in the y direction.\n",
        "\t\t\t\trowstart += jump_rows\n",
        "\t\t\telse:\n",
        "\t\t\t\t# Otherwise, shift the window down by one pixel.\n",
        "\t\t\t\trowstart += 1\n",
        "\n",
        "\t\t\t# Return a 3D array of the patches with the patch index as the first\n",
        "\t\t\t# dimension (so that patch pixels stay contiguous in memory, in a \n",
        "\t\t\t# C-ordered array).\n",
        "\n",
        "\t\treturn np.concatenate([pat[np.newaxis, ...] for pat in patches], axis=0)\n",
        "\n",
        "\tdef preprocess(self, images):\n",
        "\n",
        "\t\tnew_images = []\n",
        "\n",
        "\t\tfor i, img in enumerate(images):\n",
        "\n",
        "\t\t\tclahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(40,40))\n",
        "\t\t\timg = clahe.apply(img)\n",
        "\n",
        "\t\t\timg = cv2.bilateralFilter(img, -1, 20, 20)\n",
        "\n",
        "\t\t\tintensities = medfilt(img, (21, 21))\n",
        "\t\t\tintensities = intensities.astype(np.float32)\n",
        "\t\t\tintensities_smoothed = cv2.bilateralFilter(intensities, -1, 70, 13)\n",
        "\t\t\twidth, height = img.shape\n",
        "\t\t\timg[0:width, 0:height] = img[0:width, 0:height] + (90) - intensities_smoothed[0:width, 0:height]\n",
        "\t\t\tidx = img[:] > 210\n",
        "\t\t\timg[idx] = 18\n",
        "\n",
        "\t\t\tclahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(1,1))\n",
        "\t\t\timg = clahe.apply(img)\n",
        "\n",
        "\t\t\tnew_images.append(img)\n",
        "\n",
        "\t\treturn np.array(new_images)\n",
        "\n",
        "\tdef eval_test_img(self, sess, idx, img, gt):\n",
        "\n",
        "\t\ttest = img[np.newaxis, ... ]\n",
        "\n",
        "\t\tpred = sess.run(self.output, feed_dict={\n",
        "\t\t\t\t\t\t\tself.imgs: test, \n",
        "\t\t\t\t\t\t\tself.train_mode: False\n",
        "\t\t\t\t\t\t})\n",
        "\t\tprint('TEST_set[', idx, ']:')\n",
        "\t\tprint('gr_t: ', gt)\n",
        "\t\tprint('pred: ', pred)\n",
        "\n",
        "\tdef exctract_disc_patches(self, images, ground_truth):\n",
        "\t\t\"\"\" Exctract patches with zero distance from otic disc. \"\"\"\n",
        "\t\tpatches = []\n",
        "\t\timg_h, img_w = images[0].shape\n",
        "\t\tfor i, img in enumerate(images):\n",
        "\n",
        "\t\t\tcenter_x = int(ground_truth[i,1])\n",
        "\t\t\tcenter_y = int(ground_truth[i,0])\n",
        "\n",
        "\t\t\thalf_win_h = 22\n",
        "\t\t\thalf_win_w = 21\n",
        "\n",
        "\t\t\tx1 = center_x - half_win_w\n",
        "\t\t\ty1 = center_y - half_win_h\n",
        "\n",
        "\t\t\tif x1 < 0:\n",
        "\t\t\t\tx1 = 0\n",
        "\t\t\tif y1 < 0:\n",
        "\t\t\t\ty1 = 0\n",
        "\t\t\tif (x1 + 42) >= img_w:\n",
        "\t\t\t\tx1 = img_w - 42\n",
        "\t\t\tif (y1 + 45) >= img_h:\n",
        "\t\t\t\ty1 = img_h - 45\n",
        "\n",
        "\t\t\tx2 = x1 + 42\n",
        "\t\t\ty2 = y1 + 45\n",
        "\n",
        "\t\t\tpatch = img[y1:y2, x1:x2]\n",
        "\n",
        "\t\t\tpatches.append(patch)\n",
        "\t\t\t#patches.append(ndimage.rotate(patch, 180))\n",
        "\n",
        "\t\treturn np.array(patches)\n",
        "\n",
        "\tdef load_data(self, images_folder, gt_file):\n",
        "\t\timages = []\n",
        "\t\tfor filename in os.listdir(images_folder):\n",
        "\t\t\timg = imread(os.path.join(images_folder, filename), mode='L')\n",
        "\t\t\tif img is not None:\n",
        "\t\t\t\timages.append(imresize(img, (201, 233)))\n",
        "\t\ttrain_images = np.array(images)\n",
        "\n",
        "\t\ttrain_output = np.loadtxt(gt_file)\n",
        "\n",
        "\t\t# delete images without annotations (values (-1, -1))\n",
        "\t\tindices_to_delete = ~(train_output==-1).any(1)\n",
        "\t\ttrain_images = train_images[indices_to_delete]\n",
        "\t\ttrain_output = train_output[indices_to_delete]\n",
        "\n",
        "\t\t# resize output\n",
        "\t\ttrain_output = np.rint(train_output / 3)\n",
        "\n",
        "\t\treturn train_images, train_output\n",
        "\n",
        "\tdef make_patches(self, images, gt):\n",
        "\t\ttrain_patches = []\n",
        "\t\tpatch_dists = []\t# ground truth distance of a patch from the optic disc\n",
        "\t\tprint(\"Creating patches...\")\n",
        "\t\tfor img_id, img in enumerate(images):\n",
        "\n",
        "\t\t\t# the less overlap allowed, the more patches created\n",
        "\t\t\tpatches = self.extract_patches(img, (45, 42), overlap_allowed=0.2, cropvalue=None, crop_fraction_allowed=0.1)\n",
        "\t\t\ttrain_patches.extend(patches)\n",
        "\n",
        "\t\t\tfor patch_id, patch in enumerate(patches):\n",
        "\t\t\t\tpatch_x2d = patch_id % 24 # patches in a row\n",
        "\t\t\t\tpatch_y2d = patch_id / 24 # patches in a column              \n",
        "\t\t\t\tmid_x = patch_x2d * 8 + 21 # column step + width/2\n",
        "\t\t\t\tmid_y = patch_y2d * 9 + 23 # row step + height/2\n",
        "\n",
        "\t\t\t\tx_offset = gt[img_id, 1] - mid_x\n",
        "\t\t\t\ty_offset = gt[img_id, 0] - mid_y\n",
        "\n",
        "\t\t\t\toffset = np.array([y_offset, x_offset])\n",
        "\t\t\t\tpatch_dists.append(offset)\n",
        "\n",
        "\t\treturn np.array(train_patches), np.array(patch_dists)\n",
        "\n",
        "\tdef shuffle(self, images, gt):\n",
        "\t\tshuffle = list(zip(images, gt))\n",
        "\t\trandom.shuffle(shuffle)\n",
        "\t\timages, gt = zip(*shuffle)\n",
        "\n",
        "\t\treturn np.array(images), np.array(gt)\n",
        "\n",
        "\tdef prepare_test_data(self):\n",
        "\t\timages_folder = './images/test/'\n",
        "\t\tground_truth = './images/gt_test.txt'\n",
        "\n",
        "\t\ttrain_images, train_output = self.load_data(images_folder, ground_truth)\n",
        "\t\ttrain_images = self.preprocess(train_images)\n",
        "\n",
        "\t\tdisc_patches = self.exctract_disc_patches(train_images, train_output)\n",
        "\t\tzero_dists = np.zeros((len(disc_patches), 2))\n",
        "\t\tprint('Disc patches: ', np.shape(disc_patches))\n",
        "\n",
        "\t\ttrain_images, train_output = self.make_patches(train_images, train_output)\n",
        "\t\tprint('Orig patches: ', np.shape(train_images))\n",
        "\n",
        "\t\ttrain_output = np.concatenate((train_output, zero_dists), axis=0)\n",
        "\t\ttrain_images = np.concatenate((train_images, disc_patches), axis=0)\n",
        "\n",
        "\t\t# randomly shuffle train array\n",
        "\t\ttrain_images, train_output = self.shuffle(train_images, train_output)\n",
        "\n",
        "\t\tprint(\"Train samples:\", np.shape(train_images))\n",
        "\t\tprint(\"Output samples:\", np.shape(train_output))\n",
        "\n",
        "\t\t# create tensors\n",
        "\t\ttrain_output = train_output[:, np.newaxis, np.newaxis, : ]\n",
        "\t\ttrain_images = train_images[..., np.newaxis]\n",
        "\n",
        "\t\treturn train_images, train_output\n",
        "\n",
        "\tdef train(self, images_folder, ground_truth):\n",
        "\n",
        "\t\tprint('train(): Images processing')\n",
        "\n",
        "\t\ttrain_images, train_output = self.load_data(images_folder, ground_truth)\n",
        "\t\ttrain_images = self.preprocess(train_images)\n",
        "\n",
        "\t\tdisc_patches = self.exctract_disc_patches(train_images, train_output)\n",
        "\t\tzero_dists = np.zeros((len(disc_patches), 2))\n",
        "\t\tprint('Disc patches: ', np.shape(disc_patches))\n",
        "\n",
        "\t\ttrain_images, train_output = self.make_patches(train_images, train_output)\n",
        "\t\tprint('Orig patches: ', np.shape(train_images))\n",
        "\n",
        "\t\ttrain_output = np.concatenate((train_output, zero_dists), axis=0)\n",
        "\t\ttrain_images = np.concatenate((train_images, disc_patches), axis=0)\n",
        "\n",
        "\t\t# randomly shuffle train array\n",
        "\t\ttrain_images, train_output = self.shuffle(train_images, train_output)\n",
        "\n",
        "\t\tprint(\"Train samples:\", np.shape(train_images))\n",
        "\t\tprint(\"Output samples:\", np.shape(train_output))\n",
        "\n",
        "\t\t# create tensors\n",
        "\t\ttrain_output = train_output[:, np.newaxis, np.newaxis, : ]\n",
        "\t\ttrain_images = train_images[..., np.newaxis]\n",
        "\n",
        "\t\ttrain_set = train_images\n",
        "\t\ttrain_y = train_output\n",
        "\t\tprint(\"train_y: \", np.shape(train_y))\n",
        "\t\tprint(\"train_set: \", np.shape(train_set))\n",
        "\n",
        "\t\ttest_images, test_output = self.prepare_test_data()\n",
        "\t\ttest_set = test_images[:130, ...]\n",
        "\t\ttest_y = test_output[:130, ...]\n",
        "\t\tprint(\"test_y: \", np.shape(test_y))\n",
        "\t\tprint(\"test_set: \", np.shape(test_set))\n",
        "\n",
        "\t\ty = tf.placeholder(\"float32\", [None, 1, 1, 2])\n",
        "\n",
        "\t\tcost = tf.reduce_mean(tf.losses.mean_squared_error(labels=y, predictions=self.output))\n",
        "\t\toptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "\t\ttraining_epochs = 150\n",
        "\t\tdisplay_step = 4\n",
        "\t\tbatch_size = 130\n",
        "\t\twith self.sess as sess:           \n",
        "\t\t\tprint('train(): Training started')\n",
        "\t\t\tsess.run(tf.global_variables_initializer())\n",
        "\n",
        "\t\t\tfor epoch in range(training_epochs):\n",
        "\n",
        "\t\t\t\tavg_cost = 0.0\n",
        "\t\t\t\ttotal_batch = int(len(train_set) / batch_size) \n",
        "\t\t\t\tx_batches = np.array_split(train_set, total_batch)\n",
        "\t\t\t\ty_batches = np.array_split(train_y, total_batch)\n",
        "\n",
        "\t\t\t\tfor i in range(total_batch):\n",
        "\n",
        "\t\t\t\t\tbatch_x, batch_y = x_batches[i], y_batches[i]\n",
        "\n",
        "\t\t\t\t\t_, c = sess.run([optimizer, cost], \n",
        "\t\t\t\t\t\t\t\t\tfeed_dict={\n",
        "\t\t\t\t\t\t\t\t\t\tself.imgs: batch_x, \n",
        "\t\t\t\t\t\t\t\t\t\ty: batch_y, \n",
        "\t\t\t\t\t\t\t\t\t\tself.train_mode: True\n",
        "\t\t\t\t\t\t\t\t\t})\n",
        "\t\t\t\t\tavg_cost += c / total_batch\n",
        "\n",
        "\t\t\t\tif epoch % display_step == 0:\n",
        "\t\t\t\t\tprint(\"\\nEpoch:\", '%04d' % (epoch+1), \"\\nmse(train_set)=\", \\\n",
        "\t\t\"{:.9f}\".format(avg_cost))\n",
        "\n",
        "\t\t\t\t\tpred_y = sess.run(self.output, \n",
        "\t\t\t\t\t\t\t\t\t  feed_dict={\n",
        "\t\t\t\t\t\t\t\t\t\t   self.imgs: test_set,\n",
        "\t\t\t\t\t\t\t\t\t\t   self.train_mode: False\n",
        "\t\t\t\t\t\t\t\t\t   })\n",
        "\t\t\t\t\tmse = tf.reduce_mean(tf.square(pred_y - test_y))\n",
        "\t\t\t\t\tprint(\"MSE(test_set): %.4f\" % sess.run(mse)) \n",
        "\n",
        "\t\t\t\t\tself.eval_test_img(sess, 90, test_set[90], test_y[90])\n",
        "\t\t\t\t\tself.eval_test_img(sess, 100, test_set[100], test_y[100])\n",
        "\t\t\t\t\tself.eval_test_img(sess, 20, test_set[20], test_y[20])\n",
        "\n",
        "\t\t\tself.saver.save(sess, './model')\n",
        "\n",
        "\tdef fine_tune(self, images_folder, ground_truth):\n",
        "\t\t\"\"\" Fine tuning with patches containing disc patches (400),\n",
        "\t\t    and with normal patches (1000). This helps the model to\n",
        "\t\t    converge and stay on the position of otpic disc, if found.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\ttrain_images, train_output = self.load_data(images_folder, ground_truth)\n",
        "\t\ttrain_images = self.preprocess(train_images)\n",
        "\n",
        "\t\tdisc_patches = self.exctract_disc_patches(train_images, train_output)\n",
        "\t\tzero_dists = np.zeros((len(disc_patches), 2))\n",
        "\t\tprint('Disc patches: ', np.shape(disc_patches))\n",
        "\n",
        "\t\ttrain_images, train_output = self.make_patches(train_images, train_output)\n",
        "\t\tprint('Orig patches: ', np.shape(train_images))\t\n",
        "\n",
        "\t\ttrain_images, train_output = self.shuffle(train_images, train_output)\n",
        "\n",
        "\t\tprint(\"Train samples:\", np.shape(train_images))\n",
        "\t\tprint(\"Output samples:\", np.shape(train_output))\n",
        "\n",
        "\t\ttrain_images = train_images[:1000, ...]\n",
        "\t\ttrain_output = train_output[:1000, ...]\n",
        "\n",
        "\t\ttrain_output = np.concatenate((train_output, zero_dists), axis=0)\n",
        "\t\ttrain_images = np.concatenate((train_images, disc_patches), axis=0)\n",
        "\n",
        "\t\ttrain_images, train_output = self.shuffle(train_images, train_output)\n",
        "\n",
        "\t\t# create tensors\n",
        "\t\ttrain_y = train_output[:, np.newaxis, np.newaxis, : ]\n",
        "\t\ttrain_set = train_images[..., np.newaxis]\n",
        "\n",
        "\t\tprint(\"train_y: \", np.shape(train_y))\n",
        "\t\tprint(\"train_set: \", np.shape(train_set))\n",
        "\n",
        "\t\ty = tf.placeholder(\"float32\", [None, 1, 1, 2])\n",
        "\n",
        "\t\tcost = tf.reduce_mean(tf.losses.mean_squared_error(labels=y, predictions=self.output))\n",
        "\t\toptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "\t\ttraining_epochs = 50\n",
        "\t\tdisplay_step = 4\n",
        "\t\tbatch_size = 130\n",
        "\n",
        "\t\tprint('Starting session')\n",
        "\t\twith self.sess as sess:           \n",
        "\t\t\tsess.run(tf.global_variables_initializer())\n",
        "\t\t\tprint('train(): Training started')\n",
        "\t\t\tself.saver.restore(sess, './model')\n",
        "\t\t\tprint('Model restored')\n",
        "\n",
        "\t\t\tfor epoch in range(training_epochs):\n",
        "\n",
        "\t\t\t\tavg_cost = 0.0\n",
        "\t\t\t\ttotal_batch = int(len(train_set) / batch_size) \n",
        "\n",
        "\t\t\t\tx_batches = np.array_split(train_set, total_batch)\n",
        "\t\t\t\ty_batches = np.array_split(train_y, total_batch)\n",
        "\n",
        "\t\t\t\tfor i in range(total_batch):\n",
        "\n",
        "\t\t\t\t\tbatch_x, batch_y = x_batches[i], y_batches[i]\n",
        "\n",
        "\t\t\t\t\t_, c = sess.run([optimizer, cost], \n",
        "\t\t\t\t\t\t\t\t\tfeed_dict={\n",
        "\t\t\t\t\t\t\t\t\t\tself.imgs: batch_x, \n",
        "\t\t\t\t\t\t\t\t\t\ty: batch_y, \n",
        "\t\t\t\t\t\t\t\t\t\tself.train_mode: True\n",
        "\t\t\t\t\t\t\t\t\t})\n",
        "\t\t\t\t\tavg_cost += c / total_batch\n",
        "\n",
        "\t\t\t\tif epoch % display_step == 0:\n",
        "\t\t\t\t\tprint(\"\\nEpoch:\", '%04d' % (epoch+1), \"\\nmse(train_set)=\", \\\n",
        "\t\t\"{:.9f}\".format(avg_cost))\n",
        "\n",
        "\t\t\tself.saver.save(sess, './model_tuned')\n",
        "\n",
        "\tdef detect(self, image_file):\n",
        "\n",
        "\t\timage_orig = cv2.imread(image_file, 0)\n",
        "\t\timage_orig = imresize(image_orig, (201, 250))\n",
        "\n",
        "\t\t\"\"\" Preprocessing \"\"\"\n",
        "\t\tclahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(40,40))\n",
        "\t\timg = clahe.apply(image_orig)\n",
        "\n",
        "\t\timg = cv2.bilateralFilter(img, -1, 20, 20)\n",
        "\n",
        "\t\tintensities = medfilt(img, (21, 21))\n",
        "\t\tintensities = intensities.astype(np.float32)\n",
        "\t\tintensities_smoothed = cv2.bilateralFilter(intensities, -1, 70, 13)\n",
        "\t\twidth, height = img.shape\n",
        "\t\timg[0:width, 0:height] = img[0:width, 0:height] + (90) - intensities_smoothed[0:width, 0:height]\n",
        "\t\tidx = img[:] > 210\n",
        "\t\timg[idx] = 18\n",
        "\n",
        "\t\tclahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(1,1))\n",
        "\t\timg = clahe.apply(img)\n",
        "\n",
        "\t\timg_h, img_w = img.shape\n",
        "\t\tpatch_width = 42\n",
        "\t\tpatch_height = 45\n",
        "\t\tstart_idx = 60\n",
        "\t\tstart_idy = 40\n",
        "\n",
        "\t\tinput_img = img[start_idy:start_idy+patch_height, start_idx:start_idx+patch_width].copy()\n",
        "\t\tinput_img = input_img[np.newaxis, ..., np.newaxis]\n",
        "\n",
        "\t\titers = 20\n",
        "\t\twith self.sess as sess:\n",
        "\t\t\tself.saver.restore(sess, './model_tuned')\n",
        "\t\t\tprint('After displaying the image, press any key to continue. (', iters, ' steps)')\n",
        "\t\t\tfor i in range(iters):\t\t\t\t\n",
        "\t\t\t\tprint('Iter: ', i, '/', iters-1)\n",
        "\n",
        "\t\t\t\tpred = sess.run(self.output, \n",
        "\t\t\t\t\t            feed_dict={\n",
        "\t\t\t\t\t\t\t\t\t      self.imgs: input_img, \n",
        "\t\t\t\t\t\t\t\t\t      self.train_mode: False\n",
        "\t\t\t\t\t\t\t\t          })\n",
        "\n",
        "\t\t\t\tstart_idx = start_idx + int(pred[0,0,0,1])\n",
        "\t\t\t\tstart_idy = start_idy + int(pred[0,0,0,0])\n",
        "\t\t\t\tif start_idx < 0:\n",
        "\t\t\t\t\tstart_idx = 0\n",
        "\t\t\t\tif start_idy < 0:\n",
        "\t\t\t\t\tstart_idy = 0\n",
        "\t\t\t\tif (start_idx + patch_width) >= img_w:\n",
        "\t\t\t\t\tstart_idx = img_w - patch_width\n",
        "\t\t\t\tif (start_idy + patch_height) >= img_h:\n",
        "\t\t\t\t\tstart_idy = img_h - patch_height\n",
        "\t\t\t\tend_idx = start_idx + patch_width\n",
        "\t\t\t\tend_idy = start_idy + patch_height\n",
        "\n",
        "\t\t\t\tinput_img = img[start_idy:end_idy, start_idx:end_idx].copy()\n",
        "\t\t\t\tinput_img = input_img[np.newaxis, ..., np.newaxis]\n",
        "\t\t\t\t\n",
        "\t\t\t\timg_show = image_orig.copy()\n",
        "\t\t\t\tcv2.imshow('detection', cv2.rectangle(img_show,(start_idx,start_idy),(end_idx,end_idy),(0,255,0),1))\n",
        "\t\t\t\tcv2.waitKey(0)\n",
        "\n",
        "\tdef eval_model(self, images_folder, gt_file):\n",
        "\t\timages = []\n",
        "\t\tfor filename in os.listdir(images_folder):\n",
        "\t\t\timg = cv2.imread(os.path.join(images_folder, filename), 0)\n",
        "\t\t\tif img is not None:\n",
        "\t\t\t\timages.append(imresize(img, (201, 250)))\t\n",
        "\t\ttrain_images = np.array(images)\n",
        "\n",
        "\t\ttrain_images = self.preprocess(train_images)\n",
        "\t\t\t\t\t\t\n",
        "\t\timg_h, img_w = images[0].shape\n",
        "\t\tprint(img_h, img_w)\n",
        "\n",
        "\t\twith self.sess as sess:\n",
        "\t\t\tself.saver.restore(sess, './model_tuned')\n",
        "\t\t\toutput = []\n",
        "\t\t\tpatch_width = 42\n",
        "\t\t\tpatch_height = 45\n",
        "\n",
        "\t\t\timages_cnt = len(train_images)\n",
        "\t\t\tfor img in train_images:\n",
        "\t\t\t\t\n",
        "\t\t\t\tstart_idx = 60\n",
        "\t\t\t\tstart_idy = 40\n",
        "\t\t\t\tinput_img = img[start_idy:start_idy+patch_height, start_idx:start_idx+patch_width]\n",
        "\t\t\t\tinput_img = input_img[np.newaxis, ..., np.newaxis]\n",
        "\n",
        "\t\t\t\titers = 15\n",
        "\t\t\t\tfor i in range(iters):\t\t\t\t\n",
        "\t\t\t\t\tpred = sess.run(self.output, \n",
        "\t\t\t\t\t\t            feed_dict={\n",
        "\t\t\t\t\t\t\t\t\t\t      self.imgs: input_img, \n",
        "\t\t\t\t\t\t\t\t\t\t      self.train_mode: False\n",
        "\t\t\t\t\t\t\t\t\t          })\n",
        "\n",
        "\t\t\t\t\tstart_idx = start_idx + int(pred[0,0,0,1])\n",
        "\t\t\t\t\tstart_idy = start_idy + int(pred[0,0,0,0])\n",
        "\t\t\t\t\tif start_idx < 0:\n",
        "\t\t\t\t\t\tstart_idx = 0\n",
        "\t\t\t\t\tif start_idy < 0:\n",
        "\t\t\t\t\t\tstart_idy = 0\n",
        "\t\t\t\t\tif (start_idx + patch_width) >= img_w:\n",
        "\t\t\t\t\t\tstart_idx = img_w - patch_width\n",
        "\t\t\t\t\tif (start_idy + patch_height) >= img_h:\n",
        "\t\t\t\t\t\tstart_idy = img_h - patch_height\n",
        "\t\t\t\t\tend_idx = start_idx + patch_width\n",
        "\t\t\t\t\tend_idy = start_idy + patch_height\n",
        "\n",
        "\t\t\t\t\tinput_img = img[start_idy:end_idy, start_idx:end_idx]\n",
        "\t\t\t\t\tinput_img = input_img[np.newaxis, ..., np.newaxis]\n",
        "\n",
        "\t\t\t\tx_final = start_idx + 21\n",
        "\t\t\t\ty_final = start_idy + 23\n",
        "\n",
        "\t\t\t\toutput.append(np.array([y_final, x_final]))\n",
        "\n",
        "\t\t\toutput = np.array(output)\n",
        "\n",
        "\t\t\tground_truth = np.loadtxt(gt_file)\n",
        "\n",
        "\t\t\ttotal = 0\n",
        "\t\t\tradius_10 = 0\n",
        "\t\t\tradius_20 = 0\n",
        "\t\t\tfor i, x in enumerate(ground_truth):\n",
        "\t\t\t\td = math.sqrt((x[0]-output[i,0])**2 + ((x[1]-output[i,1])**2))\n",
        "\t\t\t\ttotal += d\n",
        "\t\t\t\tif d < 10:\n",
        "\t\t\t\t\tradius_10 += 1 \n",
        "\t\t\t\tif d < 20:\n",
        "\t\t\t\t\tradius_20 += 1\n",
        "\n",
        "\t\t\ttotal = total / images_cnt\n",
        "\t\t\tprint('Avg. distance from gt: ', total)\n",
        "\t\t\tprint('Detections with distance under 10: ', radius_10)\n",
        "\t\t\tprint('Detections with distance under 20: ', radius_20)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id98cZTJwKOq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "44767ecb-72d0-4c0d-ac3f-3f5f694ede4c"
      },
      "source": [
        "def parseArguments():\n",
        "\timport argparse\n",
        "\tparser = argparse.ArgumentParser()\n",
        "\tparser.add_argument('--train', help='Train mode.', action='store_true')\n",
        "\tparser.add_argument('--detect', help='Single image detection.', action='store_true')\n",
        "\tparser.add_argument('--finetune', help='Fine tuning the trainedmodel.', action='store_true')\n",
        "\tparser.add_argument('--eval', help='Final evaluation on imageret database.', action='store_true')\n",
        "\targs = parser.parse_args()\n",
        "\treturn args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tsess = tf.Session()\n",
        "\n",
        "\tdetector = optic_disc_detector(sess)\n",
        "\n",
        "\targs = parseArguments()\n",
        "\n",
        "\tif args.train is True:\n",
        "\t\tdetector.train('.images/train/', '.images/gt_train.txt')\n",
        "\telif args.finetune is True:\n",
        "\t\tdetector.fine_tune('./images/train/', './images/gt_train.txt')\n",
        "\telif args.detect is True:\n",
        "\t\tdetector.detect('./test_image.png')\n",
        "\telif args.eval is True:\n",
        "\t\tdetector.eval_model('./images', './images/imageret_gt.txt')\n",
        "\telse:\n",
        "\t\tprint('No argument set.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "convlayers(): Initializing layers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-50f1e903888b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptic_disc_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-765f2eda40ca>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess)\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-765f2eda40ca>\u001b[0m in \u001b[0;36mconvlayers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1_1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhe_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weights1_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \t\t\tbiases = tf.Variable(tf.constant(0.0, shape=[8], dtype=tf.float32),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable weights1_1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-6-f59beb91cb35>\", line 21, in convlayers\n    kernel = tf.get_variable(initializer=tf.keras.initializers.he_normal(), shape=[3, 3, 1, 8], name='weights1_1')\n  File \"<ipython-input-6-f59beb91cb35>\", line 6, in __init__\n    self.convlayers()\n  File \"<ipython-input-6-f59beb91cb35>\", line 646, in <module>\n    detector = optic_disc_detector(sess)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n"
          ]
        }
      ]
    }
  ]
}